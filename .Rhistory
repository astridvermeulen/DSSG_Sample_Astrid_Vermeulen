return(0) # If you violate the constraint = return a very low number
} else {
return (obj) # Maximization by default
}
} else{
return(0)
}
}
#Model GA
# popsize and maxiter very low to avoid time out error
result <- ga(type = "binary",
fitness = evaluate,
popSize = 150,
maxiter = 100,
pmutation = 0.01,
monitor = FALSE,
nBits = team_size_all) # team size of whole available team
summary(result)
# Get selected drivers
rider_selection <- summary(result)$solution[1,] %>% as.numeric()
rider_selection
# Show solution
team_all[rider_selection == 1,]
# Select team, year and race type
team = 'Etixx - Quick Step'
year = 2016
race_category = 'Flat'
# Get the whole available team in a given year
team_all <- basetable %>% filter((Team.x == team) & (Year == year)) %>% distinct(Rider,.keep_all = T) %>%     select(Rider,Team.x) # Select the available team members of a given team in a given year
# Team size is used later on in GA algorithm
team_size_all <- nrow(team_all)
# Summarize function to make the same basetable the models were trained on
summarize_function <- function(team_selection, race_category){
team_selection_name <- team_all[team_selection == 1,'Rider'] # Get names of selected riders
team_selection_base <- basetable %>% distinct(Rider ,.keep_all = T) %>% filter(Rider %in% team_selection_name) # Get information on the selected riders. The columns which we summarize in the next step are static. So it doesn't matter which race we get the riders information from.
# Calculate team scores
team <- team_selection_base %>%
summarise( team_popularity = sum(Popularity), team_potential = sum(Potential),
team_score_flat = sum(FLAT), team_score_mountain = sum(MOUNTAIN), team_score_downhill = sum(DOWNHILL),
team_score_cobbles = sum(COBBLES), team_score_tt = sum(TT), team_score_prologue = sum(PROLOGUE),
team_score_sprint = sum(SPRINT), team_score_acceleration = sum(ACCELERATION), team_score_endurance = sum(ENDURANCE),
team_score_resistance = sum(RESISTANCE), team_score_recup = sum(RECUP), team_score_hill = sum(HILL),
team_score_attack = sum(ATTACK),
avg_team_size = mean(Size), avg_team_weight = mean(Weight),
nr_drivers = n_distinct(Rider),
nr_flat_drivers = sum(flat_driver), nr_mountain_drivers = sum(mountain_driver), nr_downhill_drivers = sum(downhill_driver),
nr_cobbles_drivers = sum(cobbles_driver), nr_tt_drivers = sum(tt_driver), nr_prologue_drivers = sum(prologue_driver),
nr_sprint_drivers = sum(sprint_driver), nr_acceleration_drivers = sum(acceleration_driver), nr_resistance_drivers =
sum(resistance_driver), nr_endurance_drivers = sum(endurance_driver), nr_recup_drivers = sum(recup_driver),
nr_hill_drivers = sum(hill_driver), nr_attack_drivers = sum(attack_driver), avg_age_drivers = mean(age),
top20_finishes = sum(Pos <= 20), top10_finishes = sum(Pos <= 10), top5_finishes = sum(Pos <= 5), top3_finishes = sum(Pos <= 3)
)
#drop team points
# drop <- c("team_points")
# team = team[,!(names(team) %in% drop)]
# add race category as factor to team
# lelijke code, maar R deed moeilijk
# team[colnames(dummies)] <- as.character(0)
# category <- race_category
# category <- paste('Race_Category_' ,category, sep="")
# team[category] <- as.character(1)
# team[2,] <- 1
# team[3,] <- 0
# team <- team %>% mutate_if(is.character, as.factor)
# team <- team[-(2:3),]
# add race category as numeric to team
team[colnames(dummies)] <- as.numeric(0)
category <- race_category
category <- paste('Race_Category_' ,category, sep="")
team[category] <- as.character(1)
return(team)
}
# GA using RF to build ensemble
# Make evaluate function
evaluate <- function (x) {
team <- summarize_function(x,race_category) # from driver selection, create team scores
if(!any(is.na(team))){ # When input is all zero (no riders) immediately return low number, otherwise RF will throw error as the summarize_function will return an Na's which RF cannot handle
# predict
obj <- predict(rFmodel,team, type = "prob")[,2] # predict prob of success = objective function
team_size = sum(x)
if(team_size > 6) {
return(0) # If you violate the constraint = return a very low number
} else {
return (obj) # Maximization by default
}
} else{
return(0)
}
}
#Model GA
# popsize and maxiter very low to avoid time out error
result <- ga(type = "binary",
fitness = evaluate,
popSize = 200,
maxiter = 150,
pmutation = 0.01,
monitor = FALSE,
nBits = team_size_all) # team size of whole available team
summary(result)
# Get selected drivers
rider_selection <- summary(result)$solution[1,] %>% as.numeric()
rider_selection
# Show solution
team_all[rider_selection == 1,]
# GA using RF to build ensemble
# Make evaluate function
evaluate <- function (x) {
team <- summarize_function(x,race_category) # from driver selection, create team scores
if(!any(is.na(team))){ # When input is all zero (no riders) immediately return low number, otherwise RF will throw error as the summarize_function will return an Na's which RF cannot handle
# predict
obj <- predict(rFmodel,team, type = "prob")[,2] # predict prob of success = objective function
team_size = sum(x)
if(team_size != 6) {
return(0) # If you violate the constraint = return a very low number
} else {
return (obj) # Maximization by default
}
} else{
return(0)
}
}
#Model GA
# popsize and maxiter very low to avoid time out error
result <- ga(type = "binary",
fitness = evaluate,
popSize = 200,
maxiter = 150,
pmutation = 0.01,
monitor = FALSE,
nBits = team_size_all) # team size of whole available team
summary(result)
#install packages
if (!require("pacman")) install.packages("pacman")
require("pacman", character.only = TRUE, quietly = TRUE)
p_load(tidyverse)
library("readxl")
p_load(lubridate)
library('stringr')
set.seed(123)
p_load(dummy)
first_cycling_data <- read.csv("Races_WITHindex.csv", header = TRUE)
pro_cycling_data <- read_excel("RiderData.xlsx")
#impute missing UCIs with 0 as missing values means that the driver finished outside of the points and thus did not earn any
first_cycling_data$UCI[is.na(first_cycling_data$UCI)] <- 0
#merge on name and team (because what if a rider switches teams?)
#merge Given Name and Family name to one column: Rider (necessary for merge)
pro_cycling_data$Rider <- str_c(pro_cycling_data$`Family Name`, ' ', pro_cycling_data$`Given Name`)
# merge two data frames by ID
basetable <- merge(first_cycling_data, pro_cycling_data,by="Rider")
#transform position column to numbers
basetable$Pos <- as.integer(basetable$Pos)
basetable$Pos[is.na(basetable$Pos)] <- 100 #give the empty values a random number that does NOT indicate success (NA means DNF or DNS)
#check missing values
colSums(is.na(basetable))
#Var_9 has a high amount of missing values, as well as VAR_16, so we will drop those
#drop columns that are not of any use
drop <- c("X.x", "Rider_ID", "Team_ID", "ID", "Family Name", "Given Name", "Name", "Team.y", "Region", "ID2", "VAR10", "VAR9", "VAR16",
"VAR25", "VAR26", "VAR23", "VAR24", "VAR29", "VAR27", "VAR28", "VAR21", "VAR22", "VAR20", "X.y", "VAR2", "VAR3",
"VAR5", "VAR6", "VAR14", "VAR15", "Type", "VAR12", "VAR13", "VAR18", "Price", "VAR19", "VAR17", "VAR8", "VAR11", "VAR30",
"VAR31", "Unnamed..1", "Icon")
basetable = basetable[,!(names(basetable) %in% drop)]
#get age of rider
basetable$Birthday <- ymd(basetable$Birthday)
basetable$age <- as.integer(floor(as.numeric(basetable$Year) - (as.numeric(format(basetable$Birthday, format = "%Y")))))
basetable$age[basetable$age > 100] <- NA
#drop birthday
drop <- c("Birthday")
basetable = basetable[,!(names(basetable) %in% drop)]
#As a tryout give riders points based on the F1 system
#If he rider comes first, he gets 10 points. If he is 2nd, 9 points
#All the way down to 1 point for place 10 till 60
#If you finish outside of top 60, 0 points
#Can be altered to be more forgiving
basetable$points <- ifelse(basetable$Pos == 1, 10,
ifelse(basetable$Pos == 2, 9,
ifelse(basetable$Pos == 3, 8,
ifelse(basetable$Pos == 4, 7,
ifelse(basetable$Pos == 5, 6,
ifelse(basetable$Pos == 6, 5,
ifelse(basetable$Pos == 7, 4,
ifelse(basetable$Pos == 8, 3,
ifelse(basetable$Pos == 9, 2,
ifelse(basetable$Pos >= 10 && basetable$Pos <= 60, 1, 0)
)
))))))))
#Define that a driver has a specialty in a specific category if he has a score in that category above 75
#Exact cutoff also up for discussion, mcategories have a range between 50 and 85 (maybe have a look at the averages)
#https://web.cyanide-studio.com/games/cycling/2021/pcm/guide/basics-specialisations/
basetable$flat_driver <- ifelse(basetable$FLAT >= 75,1,0)
basetable$mountain_driver <- ifelse(basetable$MOUNTAIN >= 75,1,0)
basetable$downhill_driver <- ifelse(basetable$DOWNHILL >= 75,1,0)
basetable$cobbles_driver <- ifelse(basetable$COBBLES >= 75,1,0)
basetable$tt_driver <- ifelse(basetable$TT >= 75,1,0)
basetable$prologue_driver <- ifelse(basetable$PROLOGUE >= 75,1,0)
basetable$sprint_driver <- ifelse(basetable$SPRINT >= 75,1,0)
basetable$acceleration_driver <- ifelse(basetable$ACCELERATION >= 75,1,0)
basetable$resistance_driver <- ifelse(basetable$RESISTANCE >= 75,1,0)
basetable$endurance_driver <- ifelse(basetable$ENDURANCE >= 75,1,0)
basetable$recup_driver <- ifelse(basetable$RECUP >= 75,1,0)
basetable$hill_driver <- ifelse(basetable$HILL >= 75,1,0)
basetable$attack_driver <- ifelse(basetable$ATTACK >= 75,1,0)
#Everything gets grouped by Race_ID here, but a Race_ID can sometimes have multiple stages
#If a Race_ID is just a one day race, stage column is 0
#Make a new Race_ID_Stage , indicating the race id and stage in one column
basetable <- basetable %>%
unite(Race_ID_Stage, Race_ID, Stage,
remove = FALSE,
sep = '_') %>%
# Replace '_' with '' if it is at the end of the line
mutate(Race_ID_Stage = gsub(', $', '', Race_ID_Stage))
#Aggregate everything so that one row equals one team per race
#Variables inlcude team scores on different attributes, number of drivers with certain attributes
#Team points in the race and amount of top_X finishes per race
team_scores_per_race <-basetable %>%
group_by(Year, Race_ID_Stage, Team.x) %>%
summarise(team_points = sum(points), team_popularity = sum(Popularity), team_potential = sum(Potential),
team_score_flat = sum(FLAT), team_score_mountain = sum(MOUNTAIN), team_score_downhill = sum(DOWNHILL),
team_score_cobbles = sum(COBBLES), team_score_tt = sum(TT), team_score_prologue = sum(PROLOGUE),
team_score_sprint = sum(SPRINT), team_score_acceleration = sum(ACCELERATION), team_score_endurance = sum(ENDURANCE),
team_score_resistance = sum(RESISTANCE), team_score_recup = sum(RECUP), team_score_hill = sum(HILL),
team_score_attack = sum(ATTACK),
avg_team_size = mean(Size), avg_team_weight = mean(Weight),
nr_drivers = n_distinct(Rider),
nr_flat_drivers = sum(flat_driver), nr_mountain_drivers = sum(mountain_driver), nr_downhill_drivers = sum(downhill_driver),
nr_cobbles_drivers = sum(cobbles_driver), nr_tt_drivers = sum(tt_driver), nr_prologue_drivers = sum(prologue_driver),
nr_sprint_drivers = sum(sprint_driver), nr_acceleration_drivers = sum(acceleration_driver), nr_resistance_drivers =
sum(resistance_driver), nr_endurance_drivers = sum(endurance_driver), nr_recup_drivers = sum(recup_driver),
nr_hill_drivers = sum(hill_driver), nr_attack_drivers = sum(attack_driver), avg_age_drivers = mean(age),
#top20_finishes = sum(Pos <= 20), top10_finishes = sum(Pos <= 10), top5_finishes = sum(Pos <= 5), top3_finishes = sum(Pos <= 3)
)
#Assign a category to each race, in line with the rider categories (done with help of stage_profiles in first cycling api and the stage profile icons on the site of firstcycling)
#In the html code you can see the name for the icon, that indicated the race category
#For the single day races we assign the category by hand
ids_single <- c("4_0", "7_0", "5_0", "8_0", "9_0", "11_0", "59_0", "58_0", "24_0", "47_0", "10_0", "1988_0", "20_0", "22_0", "1172_0", "53_0", "54_0", "75_0", "35_0", "40_0")
cats_single <- c("Flat", "Cobble", "Cobble", "Cobble", "Hill", "Hill", "Hill_UHF", "Hill", "Hill", "Cobble", "Hill_UHF", "Flat", "Flat", "Hill", "Hill", "Cobble", "Hill_UHF", "Cobble", "Hill", "Cobble")
race_cats_single <- data.frame(ids_single, cats_single)
race_cats_single <- race_cats_single %>%
rename(Race_ID_Stage = ids_single, Race_Category = cats_single)
#For the races with multiple stages we extracted the profiles from the html response, with the help of python (see Cycling API.ipynb)
race_categories <- read.csv("race_categories.csv", header = TRUE)
unique(race_categories$Category)
#Translate the danish names we extracted from FC API
#UHF = up hill finish
race_categories <- race_categories %>%
mutate(Race_Category = case_when(Category == "Flatt"   ~ "Flat", Category == "Smaakupert-MF" ~ "Hill_UHF",
Category == "Smaakupert"   ~ "Hill", Category == "Fjell" ~ "Mountain",
Category == "Fjell-MF"   ~ "Mountain_UHF", Category == "Tempo" ~ "TimeTrial",
Category == "Bakketempo"   ~ "TimeTrial", Category == "Brosten" ~ "Cobble",TRUE ~ "TimeTrial"))
#Make the same Race_ID_Stage as before
race_categories <- race_categories %>%
unite(Race_ID_Stage, Race_ID, Stage,
remove = FALSE,
sep = '_') %>%
# Replace ', ' with '' if it is at the end of the line
mutate(Race_ID_Stage = gsub(', $', '', Race_ID_Stage))
drop <- c("Race_ID", "Stage", "Category")
race_categories = race_categories[,!(names(race_categories) %in% drop)]
#Join the categories to the team_scores df
team_scores_1 <- merge(team_scores_per_race, race_cats_single,by="Race_ID_Stage")
team_scores_2 <- merge(team_scores_per_race, race_categories,by= c("Year", "Race_ID_Stage"))
team_scores_per_race <- rbind(team_scores_1, team_scores_2)
hist(team_scores_per_race$team_points)
#success is defined as when the team scores at least 10 points
team_scores_per_race$success <- ifelse(team_scores_per_race$team_points >= 10, 1, 0)
team_scores_per_race$success <- as.factor(team_scores_per_race$success)
#drop columns that are not of any use
drop <- c("Team.x", "Year", "team_points", "Race_ID", "Race_ID_Stage", "Stage")
team_scores_per_race = team_scores_per_race[,!(names(team_scores_per_race) %in% drop)]
#Make dummies from Race_category
dummies <- dummy(team_scores_per_race["Race_Category"])
#Set TimeTrial as reference category
dummies <- dummies %>%
dplyr::select(!Race_Category_TimeTrial)
dummies <- dummies %>%
mutate(across(everything(), as.factor))
team_scores_per_race <- team_scores_per_race %>%
dplyr::select(!Race_Category) %>%
bind_cols(dummies)
# Inspect class imbalance
table(team_scores_per_race$success)
# create indicators randomize order of indicators
allind <- sample(x = 1:nrow(team_scores_per_race), size = nrow(team_scores_per_race))
# split in two equal parts
trainind <- allind[1:round(length(allind) * 0.5)]
testind <- allind[(round(length(allind) * (0.5)) + 1):length(allind)]
# actual subsetting
train <- team_scores_per_race[trainind, ]
test <- team_scores_per_race[testind, ]
# Inspect class imbalance
table(train$success)
##OVERSAMPLING##
# Get indices of each group
success <- which(train$success == "1")
fail <- which(train$success == "0")
# See how many instances we would want to have equal number
# as majority_class
n_desired <- length(fail)
resampled_success <- sample(x = success, size = n_desired,
replace = TRUE)
print(resampled_success)
# Combine into one large data set
train <- train[c(resampled_success, fail),]
table(train$success)
#Model building:
##HYBRID ENSEMBLE##
#Reliable package?
p_load(hybridEnsemble) #https://rdrr.io/cran/hybridEnsemble/man/hybridEnsemble.html
ytrain <- train$success
train <- train[,names(train) != 'success']
ytest <- test$success
test <- test[,names(test) != 'success']
#GA can also be used as a combination method. Right now it is ; single best, simple mean and authority based weighting
#SV has difficulties with running (or just takes a really long time)
#NB throws an error I am not able to fix (don't know where this error is thrown, can't find it in source code)
#Other available algorithms are KF= Kernel Factory, NN= Bagged Neural Network,
#RoF= Rotation Forest and KN= Bagged K- Nearest Neighbors
# hE <- hybridEnsemble(x = train, y= ytrain, algorithms = c("LR", "RF", "AB"
#                                                           #"SV")
#                                                           ),
#                      verbose = TRUE, filter=NULL,
#                      # SV.gamma = 2^-15,
#                      # SV.cost = 2^-5,
#                      # SV.degree=2,
#                      # SV.kernel='radial'
# )
#
# predictions <- predict(object = hE, newdata=test, verbose=TRUE)
# CVhE <- CVhybridEnsemble(x = train, y= ytrain, algorithms = c("LR", "RF"), verbose = TRUE, filter=NULL)
#predictions <- predict(object = CVhE, newdata=test, verbose=TRUE)
# summary(CVhE, stat='median')
# plot(x=CVhE, ROCcurve = TRUE)
##HETEROGENOUS ENSEMBLE##
#No optimalisation of parameters done yet
#Slides mention that ideally 10 classifiers should be used in the ensemble
#Perform the individual classifier predictions on a validation set, only the final predictions should be done with the test set!
p_load(randomForest, xgboost, glmnet, AUC, e1071, FNN, catboost, fastAdaboost, lightgbm, rotationForest)
# models_function <- function(train, test, ytrain, ytest) {
#Train all the models
#LR
# On sufficiently small lambda
logreg <- cv.glmnet(x = data.matrix(train), y = ytrain, family = 'binomial',alpha = 0)
# predlr <- predict(logreg, newx = data.matrix(test),
#                        type = "response", s = 0.0003)
#
# auc_lr <- AUC::auc(AUC::roc(predlr,ytest))
#NB
NB <- naiveBayes(x= train, y= ytrain)
# predNB <- predict(NB, test, type = "raw", threshold = 0.001)[,2]
# auc_nb <- AUC::auc(roc(predNB, factor(ytest)))
#KNN
# scaling
# stdev <- sapply(train, sd)
# means <- sapply(train, mean)
# trainKNN <- data.frame(t((t(train) - means)/stdev))
# testKNN <- data.frame(t((t(test) - means)/stdev))
#
# # retrieve the indicators of the k nearest neighbors of the
# # query data
# #STILL NEED TO OPTIMIZE k!
# indicatorsKNN <- as.integer(knnx.index(data = trainKNN, query = testKNN,
#                                        k = k))
# # retrieve the actual y from the training set
# predKNNoptimal <- as.integer(as.character(ytrain[indicatorsKNN]))
# # if k > 1 then we take the proportion of 1s
# predKNNoptimal <- rowMeans(data.frame(matrix(data = predKNNoptimal,
#                                              ncol = k, nrow = nrow(testKNN))))
# # Evaluate
# AUC::auc(roc(predKNNoptimal, factor(ytest)))
#RF
rFmodel <- randomForest(x = train, y = ytrain,
ntree = 500, importance = TRUE)
# predrF <- predict(rFmodel, test, type = "prob")[, 2]
# auc_rf <- AUC::auc(AUC::roc(predrF, ytest))
#CATBoost
train_pool <- catboost.load_pool(data = data.matrix(train),
label = as.numeric(as.character(ytrain)))
# test_pool <- catboost.load_pool(data = data.matrix(test))
#label = as.numeric(as.character(ytest)))
#Default parameters
params = list(loss_function = 'Logloss',
iterations = 100,
depth = 6,
learning_rate = 0.03,
l2_leaf_reg = 3, #L2 regularization term for the leaf nodes (also in xgboost)
metric_period=10)
catboost_model <- catboost.train(train_pool,  NULL,
params = params)
#
# predcatboost <- catboost.predict(catboost_model, test_pool, prediction_type = 'Probability')
# auc_catboost <- AUC::auc(AUC::roc(predcatboost, ytest))
#ADABoost
#Put in formula notation
train_ada <- train
train_ada$y <- ytrain
# test_ada <- test
# test_ada$y <- test
ABmodel <- adaboost(y ~ ., train_ada, nIter = 50) #Default iterations
# predAB <- predict(ABmodel, test)$prob[, 2]
# auc_ab <- AUC::auc(AUC::roc(predAB, ytest))
#Light GBM
#Randomly chosen parameters
param_set <- list(num_leaves = 4,
learning_rate = 0.05, objective = "binary",
boosting = "gbdt", num_iterations = 20)
lgbm_model <- lightgbm(data = as.matrix(train), params = param_set,
label = as.numeric(as.character(ytrain)), verbose = -1)
# predlgbm <- predict(lgbm_model, as.matrix(test))
# auc_lgbm <- AUC::auc(AUC::roc(predlgbm, ytest))
#Rotation Forest
train_rof <- sapply(train, as.numeric)
RoF <- rotationForest(x = train_rof, y = ytrain, L = 100)
# predRoF <- predict(RoF, test)
# auc_rof <- AUC::auc(AUC::roc(predRoF, ytest))
#XGB
#Right now with defaults, but xgboost will perform better if hyperparamters are tuned!
train_xgb <- train %>%
mutate_if(is.factor, as.character) %>% mutate_if(is.character, as.numeric)
# test_xgb <- test %>%
#   mutate_if(is.factor, as.character) %>% mutate_if(is.character, as.numeric)
# preparing matrices
dtrain <- xgb.DMatrix(data = as.matrix(train_xgb), label = as.numeric(as.character(ytrain)))
#dtest <- xgb.DMatrix(data = as.matrix(test_xgb))
xgb <- xgb.train(data = dtrain,
objective = "binary:logistic", verbose = 0, nrounds=500)
# predxgb <- predict(xgb, dtest)
# auc_xgb <- AUC::auc(AUC::roc(predxgb, ytest))
# Weight according to performance
# finalpredictions <- (auc_lr/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predlr +
#                     (auc_nb/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predNB +
#                     (auc_rf/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predrF +
#                     (auc_catboost/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predcatboost +
#                     (auc_ab/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predAB +
#                     (auc_lgbm/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predlgbm +
#                     (auc_rof/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predRoF +
#                     (auc_xgb/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predxgb
# final_AUC <- AUC::auc(AUC::roc(finalpredictions, ytest))
#Make a function for all the predictions
predict_function <- function(test, ytest) {
predlr <- predict(logreg, newx = data.matrix(test),
type = "response", s = 0.0003)
auc_lr <- AUC::auc(AUC::roc(predlr,ytest))
predNB <- predict(NB, test, type = "raw", threshold = 0.001)[,2]
auc_nb <- AUC::auc(roc(predNB, factor(ytest)))
predrF <- predict(rFmodel, test, type = "prob")[, 2]
auc_rf <- AUC::auc(AUC::roc(predrF, ytest))
test_pool <- catboost.load_pool(data = data.matrix(test))
predcatboost <- catboost.predict(catboost_model, test_pool, prediction_type = 'Probability')
auc_catboost <- AUC::auc(AUC::roc(predcatboost, ytest))
test_ada <- test
test_ada$y <- test
predAB <- predict(ABmodel, test)$prob[, 2]
auc_ab <- AUC::auc(AUC::roc(predAB, ytest))
predlgbm <- predict(lgbm_model, as.matrix(test))
auc_lgbm <- AUC::auc(AUC::roc(predlgbm, ytest))
predRoF <- predict(RoF, test)
auc_rof <- AUC::auc(AUC::roc(predRoF, ytest))
test_xgb <- test %>%
mutate_if(is.factor, as.character) %>% mutate_if(is.character, as.numeric)
dtest <- xgb.DMatrix(data = as.matrix(test_xgb))
predxgb <- predict(xgb, dtest)
auc_xgb <- AUC::auc(AUC::roc(predxgb, ytest))
# Weight according to performance
finalpredictions <- (auc_lr/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predlr +
(auc_nb/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predNB +
(auc_rf/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predrF +
(auc_catboost/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predcatboost +
(auc_ab/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predAB +
(auc_lgbm/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predlgbm +
(auc_rof/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predRoF +
(auc_xgb/(auc_lr + auc_nb + auc_rf + auc_catboost + auc_ab + auc_lgbm + auc_rof + auc_xgb)) * predxgb
final_AUC <- AUC::auc(AUC::roc(finalpredictions, ytest))
.GlobalEnv$final_preds <- finalpredictions
.GlobalEnv$final_AUC <- final_AUC
}
#Make the final predictions
#resulting predicitions are in final_preds with a corresponding final_AUC
predict_function(test, ytest)
##HETEROGENOUS ENSEMBLE##
#No optimalisation of parameters done yet
#Slides mention that ideally 10 classifiers should be used in the ensemble
#Perform the individual classifier predictions on a validation set, only the final predictions should be done with the test set!
p_load(randomForest, xgboost, glmnet, AUC, e1071, FNN, catboost, fastAdaboost, lightgbm, rotationForest)
p_load(catboost)
library(catboost)
install(catboost)
install.packages("catboost")
p_load(catboost)
#install packages
if (!require("pacman")) install.packages("pacman")
require("pacman", character.only = TRUE, quietly = TRUE)
p_load(tidyverse)
library("readxl")
p_load(lubridate)
library('stringr')
set.seed(123)
p_load(dummy)
p_load(catboost)
install.packages('devtools')
install.packages("devtools")
install.packages("devtools")
devtools::install_url('https://github.com/catboost/catboost/releases/download/v1.0.5/catboost-R-Windows-1.0.5.tgz', INSTALL_opts = c("--no-multiarch", "--no-test-load"))# models_function <- function(train, test, ytrain, ytest) {
p_load(devtools)
#install packages
if (!require("pacman")) install.packages("pacman")
require("pacman", character.only = TRUE, quietly = TRUE)
##HETEROGENOUS ENSEMBLE##
#No optimalisation of parameters done yet
#Slides mention that ideally 10 classifiers should be used in the ensemble
#Perform the individual classifier predictions on a validation set, only the final predictions should be done with the test set!
p_load(randomForest, xgboost, glmnet, AUC, e1071, FNN, catboost, fastAdaboost, lightgbm, rotationForest)
p_load(devtools)
devtools::install_url('https://github.com/catboost/catboost/releases/download/v1.0.5/catboost-R-Windows-1.0.5.tgz', INSTALL_opts = c("--no-multiarch", "--no-test-load"))# models_function <- function(train, test, ytrain, ytest) {
